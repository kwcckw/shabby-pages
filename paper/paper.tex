\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage[colorlinks]{hyperref}
\usepackage[dvipsnames]{xcolor}
\hypersetup{
    colorlinks = false,
    linkbordercolor = {blue}
}
\usepackage{listings}
\usepackage{courier}

% check mark and x for table
\newcommand{\cmark}{{\color{ForestGreen}\ding{51}}}%
\newcommand{\xmark}{{\color{Maroon}\ding{55}}}%

\def\rot{\rotatebox}

\renewcommand{\floatpagefraction}{.8}
%
\begin{document}

\title{ShabbyPages: A Recipe for Repeatable, Synthetic, Modern Document Images}
%\author{Alexander Groleau\inst{1,2} \and
%  Kok Wei Chee\inst{1} \and
%  Stefan Larson\inst{3} \and
%  Jonathan Boarman\inst{1} \and
%  Samay Maini\inst{1}}
%
%\authorrunning{A. Groleau et al.}
%
%\institute{Sparkfish LLC (\email{augraphy@sparkfish.com})
%  \and
%  Left Associates LLC (\email{research@left.associates})
%  \and
%  Vanderbilt University}

\author{Anonymous ICDAR 2023 submission}
\titlerunning{ShabbyPages}


\maketitle

\begin{abstract}
The ShabbyPages document image dataset is produced using the Augraphy document image augmentation tool.
The development of the computation pipeline used to generate the corpus is discussed, and the results presented.
The final corpus contains over 6000 "born-digital" ground truth document images, sourced on the Web, with synthetically-noised counterparts (``shabby pages'') that appear to have been printed and faxed, photocopied, or otherwise altered through physical processes.
The release of this dataset and the recipe for its production attempts to address a growing need for labeled training document images for supervised learning tasks.
The results of initial experiments are discussed, in which the corpus is used to train performant convolutional denoisers which remove real noise features with a high degree of human-perceptible fidelity, establishing baseline performance for a new ShabbyPages benchmark.
\end{abstract}

\section{Introduction}
%Supervised learning requires a collection of training data and accompanying ``labels'', which in the graphical modality are clean images, free of noise or other degradations.
%ShabbyPages is a large dataset composed of 6202 clean/noisy pairs of images taken from 600 digital documents.
%Information about the set's contents and its relation to other datasets are explored in the next section.
%The noisy images were created from their clean origins by the application of a pipeline developed with the Augraphy library; the production of ShabbyPages is detailed in the third section, with all source code and input materials made openly available.
%The final dataset is used to train several denoising models which are compared directly and whose prediction results are compared together in the final section.

Denoising is a fundamental concern for many document processing workflows, wherein unwanted artifacts introduced to a document image via noisy processes like scanning are removed.
The effectiveness of the denoising stage of document processing pipelines has implications for downstream tasks like optical character recognition (OCR) and layout parsing [need to cite].
Recent work in supervised machine learning has yielded promising results at the denoising task [need to cite], increasing the importance of access to large volumes of high-quality training and evaluation data.

Prior work has introduced denoising and binarization datasets, but these are often small in scale, limited in the types of noise features present, not diverse enough to train robust models, and impossible to recreate.
\begin{itemize}
\item The DIBCO datasets — widely recognized as critical data for document denoising and binarization — are far too small for large-scale training; each DIBCO dataset ranges between 10-20 images total.
\item The NoisyOffice denoising dataset \cite{ref_NoisyOfficeDatabase} is also quite small, having \emph{72} training samples, and contains only a few types of noise — primarily artificial wrinkles and stains, with little variation.
\item The DDI-100 corpus \cite{ddi-100-2019} is larger in scale than NoisyOffice and the DIBCO datasets, but is again limited in the degree of feature diversity \cite{detection-masking-2022}.
\end{itemize}

In this paper, we introduce a new dataset designed primarily for the document denoising task, which addresses each of the concerns listed above.
This new dataset, \emph{ShabbyPages}, is large-scale, having \emph{6202} training and \emph{mmm} [TODO: do we need this?] testing samples.
\emph{ShabbyPages} is more diverse than alternatives like NoisyOffice, as it contains documents from various language groups and contains documents with graphical elements like tables and figures.
In the next sections, we provide motivation and context for ShabbyPages' creation, compare it at length to other important datasets in the same space, describe the corpus construction process, and finally demonstrate its utility by benchmarking document denoising models trained on the dataset.

\begin{figure}
\centering
\includegraphics[width=0.98\columnwidth, frame]{figures/matrix.png}
\caption{Sample patches from ShabbyPages.} \label{fig1}
\end{figure}

\section{The Dataset}
ShabbyPages joins a rapidly growing number of open-source datasets freely available online.
Human activity today generates a tremendous amount of data, with many people choosing to publish theirs on websites like Kaggle \cite{ref_Kaggle}, Hugging Face Hub \cite{ref_HuggingFaceHub}, and many more.
All of this data is arranged into categories: sets exist which contain text, images, audio, and data from other modalities, with each set often collected and designed for a purpose.
Of the many image datasets available, the authors understand few to have been designed for groundtruthed supervised learning of document-specific tasks like binarization or denoising.
In fact, we could find only one other set compiled specifically for doing so with images of \textit{modern} documents: NoisyOffice \cite{ref_NoisyOffice}.

\subsection{Features}
Modern artificial neural networks produce a latent representation of their training data, a space from which outputs are sampled.
In deep networks, these representations live within multiple layers, each of which corresponds to a feature of the input data.
Document images are a particularly interesting category, as the data within the documents each picture represents is itself frequently multi-modal; the document may contain an image and its description, a table with statistics, multiple regions of structured and formatted text, and even other documents, making their internal representations by neural networks often quite complicated.

Networks with a wider variety of latent representations frequently perform better when generalizing their learned functions to other tasks.
For this reason, procuring a large-enough volume of robust-enough training data is paramount.
Several sets of such images exist, many intended for processing by deep neural nets; Table 1 below contains a comparison of some of the key hyperproperties of the ShabbyPages set compared with other popular document image datasets.

%\begin{table}
%\centering
%\caption{Document Image Dataset Feature Comparison}\label{tab1}
%\begin{tabular}{|@{\hspace{1em}}l@{\qquad}|@{\hspace{1em}}l@{\qquad}|@{\hspace{1em}}l@{\qquad}|@{\hspace{1em}}l@{\qquad}|@{\hspace{1em}}l@{\qquad}|}
%\hline
%Feature & ShabbyPages & NoisyOffice & RVL-CDIP & Tobacco800 \\
%\hline
%Images & 6202 & 72 & 400000 & 1290\\
%Diversity & 0.488 & 0.317 & & \\
%Document-specific & Yes & Yes & Yes & Yes\\
%Synthetic & Yes & Yes & No & No\\
%Groundtruthed? & Yes & Yes & No & No\\
%Font size & Multiple & 16pt & Multiple & Multiple\\
%Paper styles & Multiple & 1 & Multiple & Multiple\\
%Multilingual & Yes & No & No & No\\
%Documents contain images? & Yes & No & Yes & Yes\\
%Pre-categorized & No & No & Yes & Yes\\
%\hline
%\end{tabular}
%\end{table}


% WIP: this table is a work-in-progress
\begin{table}[]
    \centering
    \caption{Summary of datasets for document binarization (above line) and document denoising (below line) tasks.}
    \label{tab:datasets}
    \begin{tabular}{lcllllllll}
        \textbf{Dataset} &
        \rot{80}{\textbf{Dataset Size}}
            & \rot{80}{\textbf{Synthetic Noise}}
            & \rot{80}{\textbf{Ground-Truths}}
            & \rot{80}{\textbf{Diversity}}
            & \rot{80}{\textbf{Mult. font sizes}}
            & \rot{80}{\textbf{Mult. paper styles}}
            & \rot{80}{\textbf{Multilingual}}
            & \rot{80}{\textbf{Contains graphics}}
            & \rot{80}{\textbf{Reproducible}}\\
            \midrule
         DIBCO-9 \cite{dibco-09} & 10 & \xmark & \cmark &  & \cmark & \cmark & \cmark & \xmark & n/a \\
         DIBCO-10 \cite{dibco-10} & 10 & \xmark & \cmark & & \cmark & \cmark & \xmark & \xmark & n/a \\
         DIBCO-11 \cite{dibco-11} & 16 & \xmark & \cmark & & \cmark & \cmark & \cmark & \xmark & n/a\\
         DIBCO-12 \cite{dibco-12} & 14 & \xmark & \cmark & & \cmark & \cmark & \xmark & \xmark & n/a \\
         DIBCO-13 \cite{dibco-13} & 16 & \xmark & \cmark & & \cmark & \cmark & \cmark & \xmark & n/a \\
         H-DIBCO-14 \cite{dibco-14} & 10 & \xmark & \cmark & & \cmark & \cmark & \cmark & \xmark & n/a \\
         H-DIBCO-16 \cite{dibco-16} & 10 & \xmark & \cmark & & \cmark & \cmark & \cmark & \xmark & n/a \\
         DIBCO-17 \cite{dibco-17} & 20 & \xmark & \cmark & & \cmark & \cmark & \cmark & \xmark & n/a \\
         H-DIBCO-18 \cite{dibco-18} & 10 & \xmark & \cmark & & \cmark & \cmark & \cmark & \xmark & n/a\\
         %DIBCO-19 \cite{dibco-19} & & \xmark & & & & & & & n/a\\ % SL: I can't find this dataset online
         Bickley Diary \cite{bickley-diary} & 7 & \xmark & \cmark &  & \xmark & \xmark & \xmark & \xmark & n/a\\
         PHIBD \cite{nafchi-2013-icdar} & 15 & \xmark & \cmark &  & \cmark & \cmark & \xmark & \xmark & n/a\\
         %TQBC & & & & & & & & & n/a\\ % SL: I can't find this dataset online
         \midrule
         NoisyOffice \cite{ref_NoisyOffice} & 72  & \cmark & \cmark & 0.317 & \cmark & \xmark & \xmark & \xmark & \xmark \\
         DocCreator \cite{ref_DocCreator} & 6,059 & \cmark & \cmark & & \cmark & \cmark & \cmark & \xmark &\cmark\\
         %LS-HDIB \cite{ls-hdib-2021} & & \cmark & & & & & & & \\
         DDI-100 \cite{ddi-100-2019} & 100k+~ & \cmark & \cmark & & & & & & \xmark\\
         ShabbyPages (ours)~ & 6,202~ & \cmark & \cmark & 0.488~ & \cmark~ & \cmark~ & \cmark & \cmark & \cmark \\
         \bottomrule
    \end{tabular}
\end{table}

\textbf{Diversity.} We use the diversity metric as defined in \cite{kang-etal-2018-data,larson-etal-2019-outlier} to help us measure intra-dataset variety.
This metric is defined as
$$
Diversity(X) = \frac{1}{|X|^2} \sum_{a\in X} \sum_{b\in X} \|f(a)-f(b)\|_2
$$
where $f(\cdot)$ is an embedding function mapping the input image into an \emph{n}-dimensional vector space.
Here, we use CLIP's ViT model \cite{clip}, which embeds each image into a 512-dimensional space.
The intuition behind the diversity metric is that datasets where images are highly similar will have lower diversity scores, and datasets where images share less visual similarity will have higher diversity scores.
A dataset with low diversity might not be representative enough of the real world, and low diversity may further correlate with task easiness.

\subsection{Statistics}
ShabbyPages contains documents from multiple classes, which contain many types of information.

Table 2 below contains some statistics for the dataset, in grayscale, at a standard resolution of 150 ppi.

\begin{table}
    \centering
    \caption{ShabbyPages dataset statistics.}\label{tab1}
    \begin{tabular}{@{\hspace{2em}}l@{\qquad}@{\hspace{2em}}l@{\qquad}}
        \toprule
        \textbf{Statistic} & \textbf{Value} \\
        \midrule
        Num. Images & 6202 \\
        DPI & 150 \\
        Max image size & 3336x12157 \\
        Min image size & 532x532\\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Other Datasets}
The most similar existing dataset to ShabbyPages for the tasks of document denoising include NoisyOffice \cite{ref_NoisyOfficeDatabase}.
NoisyOffie is a very limited, however, consisting of two types of font and three font sizes of text across four base images. The NoisyOffice dataset was created by permuting these font/size/base combinations, and thus the dataset is feature-poor.
NoisyOffice also lacks non-English and non-Latin texts, as well as graphical elements like images, logos, and tables.
To add noise to NoisyOffice, the creators used four different types of noise backgrounds ([TODO: list them here]) and then overlayed generated text onto the noisy backgrounds.
Overall, NoisyOffice consists of 72 document images.
Another dataset, DDI-100 \cite{ddi-100-2019}, was constructed in a manner similar to NoisyOffice.
DDI-100 is far larger than NoisyOffice, but has been criticized for not containing enough diversity or noise to be an effective benchmark \cite{detection-masking-2022}.
Additionally, the pipeline code for creating the noisy versions of both NoisyOffice and DDI-100 is not available, and thus these datasets cannot be reproduced.

Prior work includes several datasets for the related task of document image binarization, which seeks to convert a document image into a binary image (i.e., where text is black and background is white).
These document binarization datasets include the DIBCO datasets \cite{dibco-09,dibco-10,dibco-11,dibco-12,dibco-13,dibco-14,dibco-16,dibco-17,dibco-18}.
As summarized in Table~\ref{tab:datasets}, the DIBCO datasets are very small, ranging between 10 and 20 samples each (for a total of 106 images across DIBCO-9 through DIBCO-18).
The DIBCO datasets do not contain graphical elements like images, tables, charts, etc.
A dataset similar to the DIBCO benchmarks is the Persian Heritage Image Binarization Dataset (PHIBD) \cite{phibc-2012-nafchi,nafchi-2013-icdar}, which consists of a small number images of handwritten Persian writing.
Still others like the Bickley Diary dataset \cite{bickley-diary} and the 2019 Time-Quality Binarization Competition datasets \cite{2019-time-quality-competition} have also been used for benchmarking document binarization approaches, but these do not appear to be publicly available \cite{tensmeyer-binarization-review-2020}.
Ultimately, the sizes of these document binarization corpora severly limit the types of algorithms and models that can be used.

Other popular document image datasets include large amounts of real noise: These datasets include RVL-CDIP \cite{ref_RVL-CDIP}, Tobacco-3482 [CITE], Tobacco-800 \cite{ref_Tobacco800}, FUNSD [cite], but these datasets do not have corresponding clean ground-truth document images to train or evaluate document denoisiers.

[TODO: the following could be moved to before this Prior Work section, or into the Construction section.]

%Supervised learning tasks require labeled training data, called the ground truth.
%Much of the world's data does not come with labels, which instead must be created by humans in some fashion.
%In Table 1, we indicated that NoisyOffice was the only other groundtruthed dataset we examined.
%This isn't strictly true: there are projects to add some types of groundtruth to RVL-CDIP \cite{ref_zenodo,ref_goldmann} and Tobacco800 \cite{ref_tobacco800gt2,ref_tobacco800gt1}, but the images in these sets were produced by real scans, without an accompanying original digital document.
%Conversely, NoisyOffice contains images of real scanned documents intended for validation, as well as their digital groundtruth and synthetic training image data.

ShabbyPages includes not only the groundtruth images, but also the original digital documents used to produce them, and the software for fully reproducing the ShabbyPages set, allowing anyone to quickly produce a new set in the Shabby family, optionally re-exporting groundtruths at a different resolution beforehand.
The 6202 images in the ShabbyPages release set are really just the first sample from the space of these documents; we encourage readers to build their own.
NoisyOffice is the only other set we examined that contains its digital provenance, but the released database materials lack a means of reproducing the simulated printing method used to create the NoisyOffice training data.
Augraphy's PaperFactory augmentation is such a tool.
The other sets are not even theoretically extensible in this way.

%\subsubsection{NoisyOffice}
%The NoisyOffice database \cite{ref_NoisyOffice} contains a very low 4 base images, 2 types of font, 3 font sizes, and 4 types of noise, for a total of 72 unique images.
%The groundtruth images were produced in a standard word processor, with the simulated noisy images produced by first adding noise to a blank sheet of paper, storing that as a background image after digital scanning, then overlaying the foreground text onto the noisy background texture.
%An additional set of real NoisyOffice images was produced, where the foreground text was printed on blank white sheets, the results of which were physically augmented by the introduction of footprints, coffee stains, wrinkles, and folds.

ShabbyPages began with 600 documents sourced from the Internet from culturally-important sources.
A manifest is included in the dataset which contains an ordered table of the filenames, the languages present in the document, the download URL for the file, and an English name or acronym for the source organization or person.
These documents were split into their constituent pages, which were then passed through an Augraphy pipeline we tuned over a month or two.
This tuning amounted to tweaking numerical parameters for each of the 24 different augmentations from the Augraphy library.
All or none of these may have applied to each input image, and only one pipeline run was completed per image.
Each augmentation has a high degree of internal variability due to several calls to a pseudorandom number generator on which several transforms depend.
The conjunction of all these factors implies ShabbyPages is a sample from a massive space.
Indeed, the cosine similarity computed over OpenAI's CLIP representations of each output image averages to 0.49, much higher than NoisyOffice's 0.31.
Augraphy does not yet have augmentations for footprints or coffee stains, so the dark regions and edges of the stains and prints were simulated with features introduced by other augmentations, but Augraphy does contain the PaperFactory transformation which selects and crops a texture from a local directory of paper images.
The result of a PaperFactory application is the foreground text ``printed'' onto the background texture, similar to NoisyOffice's construction.

%\subsection{RVL-CDIP}
%The RVL-CDIP dataset \cite{ref_RVL-CDIP} consists of 400,000 grayscale images divided into 16 document classes, each with a similar number of elements.
%The dataset is pre-segmented into 320,000 training, 40,000 test, and 40,000 validation images, facilitating immediate use in machine learning applications.
%The classes represented are the following:

%\begin{center}
%letter, form, email, handwritten, advertisement, scientific report,
%scientific publication, specification, file folder, news article,
%budget, invoice, presentation, questionnaire, resume, memo.
%\end{center}

%The images exhibit poor fidelity with what we can imagine their originals to have been, contain substantial noise, and many were scanned at around 100 pixels per inch, quite a low resolution for documents.
%None of the images in the CDIP set come with groundtruths.

%The collection of RVL-CDIP is an impressive feat in its own right: nearly half a million scanned documents with real noise, already categorized.
%Augraphy makes synthetic documents, intended to simulate real noise without the need to physically produce it.
%One of the augmentations used in the ShabbyPages set is BadPhotoCopy, which mimics low-fidelity noisy or damaged scans.

%\subsection{Tobacco800}
%Tobacco800, like RVL-CDIP, is a subset of the Complex Document Image Processing collection, containing images of documents originally internal to tobacco companies, and released under the Tobacco Master Settlement Agreement.
%The corpus is composed of 1290 images, gathered and scanned at different times and by different devices.
%Several of the images in this set are consecutively-numbered pages, increasing the set's utility for certain image analysis tasks \cite{ref_Tobacco800}.
%Document images in this set vary from from 150 to 300 DPI with dimensions between 1200 pixels wide and 2500 pixels tall on the low end and 1600 by 3200 on the high.

Every document in the ShabbyPages groundtruth set is a multi-page document.
The released set was exported at 150 DPI, but the original documents are also distributed, so re-exporting at a higher resolution is trivial.
The smallest image is 532 by 532, with the largest 3336 wide by 12157 tall.
6202 images are present in ShabbyPages, but this number could easily be increased by simply running another Augraphy pipeline to generate more; in testing, we rendered several tens of thousands of such images, with the cumulative computation time still less than one day due to Augraphy's efficiency.

\section{Construction}
This section describes the dataset generation methodology; code for all of this is available on GitHub.

\subsection{Data Gathering}
A team of workers searched the public internet for PDFs of many different kinds.
600 unique documents were retrieved, totaling 6202 pages.
A similar process was completed to collect paper textures on which to "print" the documents: 300 unique textures were gathered, all of which are either in the public domain or carry CC-0, CC-BY, or CC-BY-SA licenses.
Care was taken to retrieve freely-available and attributable documents; a CSV file containing document and paper texture links (as of time of download), with their sources, is available and distributed with the dataset.
We also manually reviewed the gathered documents to verify that no personal identity information was present within the corpus.

\subsection{PDF to PNG}
The ~pdftoppm~ tool from the ~poppler-utils~ package was used to split the PDF documents into individual pages.
Each page was converted to a PNG image at 150dpi, a common printing resolution.
Because the majority of these documents were created with the standard US Letter dimensions, this resulted in the most common image dimension being 1275 pixels wide by 1650 pixels tall.

\begin{lstlisting}[language=bash]
  pdftoppm document_name.pdf document_name -png -r 150
\end{lstlisting}

\subsection{Developing the Pipeline}
The Augraphy library presents an easy-to-use API for constructing feature pipelines, which has been designed for interoperability with other augmentation tools, and within the broader data ecosystem.
While Augraphy's default pipeline has what we believe to be quite realistic defaults, we wanted a broader range of features from this dataset than those the default pipeline could produce.
To address this, we broke all of the parameters for every augmentation constructor out into separate variables, tweaking these and committing the new pipeline to GitHub.
We created an automated daily build in GitHub Actions to render a random selection of ground-truth images with the updated pipeline, then our team met frequently to discuss the output and make adjustments to the augmentation parameters, until we were consistently satisfied with the results.

The pipeline uses the following augmentations: [TODO: expand]
BadPhotocopy
BindingsAndfasteners
BleedThrough
DirtyDrum
Dirtyrollers
Dithering
Faxify
Gamma
Geometric
InkBleed
Jpeg
Letterpress
LightingGradient
LowInkPeriodicLines
LowInkRandomLines
Markup
NoiseTexturize
PageBorder
PaperColor
PaperFactory
PencilScribbles
SubtleNoise

PaperFactory was used with 300 textures, dramatically increasing the possible variations in the final dataset. We did not use Brightness or BrightnessTexturize, because these can interact poorly with some of the paper textures we chose. We also omitted the BookBinding augmentation, which can alter the document image geometry by adding additional pixels, making the result unsuitable for analysis by many common metrics.

\subsection{Processing Augraphy on a multicore system}
Execution time is dependent on which augmentations are executed at runtime; an Augraphy pipeline can take several seconds to process large images.
The library is under active development with performance enhancements underway, but the time cost to generate large datasets sequentially is prohibitive when dealing with thousands of files, so we use a multi-process pooling technique to distribute the workload across many processor execution threads.
For each process, we generate a new pipeline, run the pipeline object on an image, and save the output, using the code below: [TODO: can we cut this down to just the pipeline function?]

\begin{lstlisting}[language=Python]
    import os
    import cv2
    from multiprocessing import Pool
    from pathlib import Path

    input_path = Path("/path/to/input/images")
    filenames = [(input_path / name)
      for name
      in os.listdir(input_path)]

    pool = Pool(os.cpu_count())

    def run_pipeline(filename):
        image = cv2.imread(filename)
        # returns the current Shabby Pages pipeline
        pipeline = get_pipeline()
        data = pipeline.augment(image)
        shabby_image = data["output"]
        cv2.imwrite(filename.parent / f"{filename.stem}-shabby{filename.suffix}")

    pool.map(run_pipeline, filenames)

\end{lstlisting}

Processing all 6202 images took less than half an hour on the 64 cores of a Graviton3 c7g.16xlarge instance on AWS.

\section{Experimentation with the ShabbyPages Set}
Binarization and denoising are two important techniques for the removal of unwanted data from images.
Both approaches can be achieved by supervised learning of relationships between features of the input and output data.
In this section, we train two NAFNet \cite{ref_nafnet} instances as denoisers on both the NoisyOffice and the ShabbyPages datasets. We evaluate these models' predictions in a cross-validation test on both datasets, both by visual inspection and by metric computation.
The model was ``off-the-shelf''; besides rigging up the training inputs and choosing a reasonable training epoch limit, we made no significant alterations to the provided hyperparameters.

\subsection{Binarization} [TODO: do we want to throw this away? It's still an important part of the story]
In this section, we discuss results obtained by training a denoiser to both restore augmented images to their ground truths and to classify pixels as foreground.

We first performed an ensemble binarization preprocessing step on the clean groundtruth images.
For each image, we computed the Niblack, Sauvola, and Otsu thresholds, using these to binarize three copies of the input. The average over the resulting matrices was taken elementwise, producing the final binarized groundtruth.

One instance of the model was trained on the full SimulatedNoisyOffice corpus, while the other was trained on a 1050-image subset of ShabbyPages, cropped to a random 500x500 patch. In both cases, the prediction target was the relevant ensemble-binarized groundtruths, and each instance trained for 100 epochs.
As a test, these models were used to denoise and binarize both the NoisyOfficeReal dataset and a testing subset taken from ShabbyPages which does not overlap the training subset. [TODO: Can we get the metrics for ShabbyReal instead?]
Common image processing metrics were applied to the predictions made by each model, comparing these to the (preprocessed) groundtruths.
The averages of each metric over all such predictions was taken; these are presented in Table 2.

\begin{table}[]
    \centering
    \caption{Document image *binarization* performance of a NAFNet model trained and tested on ShabbyPages and NoisyOffice.}
    \scalebox{0.9}{
    \begin{tabular}{lll}
    \toprule
    \textbf{Training Set~~} & \textbf{Test Set} & \textbf{SSIM$\uparrow$~}\\
        \midrule
        \textsf{ShabbyPages} & \textsf{NoisyOffice-real} & 0.947\\
        \textsf{NoisyOffice-sim} & \textsf{ShabbyPages} & 0.811\\
        \bottomrule
    \end{tabular}}
    \label{tab:binarization_results}
\end{table}

The ShabbyPages-trained model was able to classify foreground and background pixels with a substantially higher degree of accuracy than the NoisyOffice-trained model, even with arguably worse training data.

\subsection{Denoising}
Digital computation has enabled humanity to produce ever-growing amounts of both data and reasons to process it.
Doing so is easiest when the data is free of unexpected outliers, the signal has less jitter, and when our aesthetic sensibilities are un-challenged.

Denoising grayscale images is a considerably harder task than binarization, as the space of possible predictions for a pixel value increases from one to eight bits (black/white to 256 shades of gray).
To offset this increased difficulty, we randomly selected ten patches of 400x400 pixels from each page, then trained another NAFNet instance on this data for 1000 epochs. The results of a denoising experiment are presented in the table below.

\begin{table}[]
    \centering
    \caption{Document image *denoising* cross-test performance of NAFNet models trained on ShabbyPages and NoisyOffice.}
    \scalebox{0.9}{
    \begin{tabular}{lll}
    \toprule
    \textbf{Training Set~~} & \textbf{Test Set} & \textbf{SSIM$\uparrow$~}\\
        \midrule
        \textsf{ShabbyPages} & \textsf{NoisyOffice-real} & 0.950\\
        \textsf{NoisyOffice-sim} & \textsf{ShabbyPages} & 0.83\\
        \bottomrule
    \end{tabular}}
    \label{tab:denoising_results}
\end{table}

As can be seen from Table \cite{tab:denoising_results}, the ShabbyPages-trained denoiser generalized to NoisyOffice far better than the NoisyOffice-trained model generalized to ShabbyPages. This result is consistent with the much higher diversity present within the ShabbyPages set, relative to features found in the NoisyOffice database.

\subsubsection{Interpretation} [TODO: roll this into the previous section and/or delete most of it]
As a baseline, we first evaluated the models against test sets associated with their training databases; the model trained on SimulatedNoisyOffice was used to enhance the RealNoisyOffice set, while the ShabbyPages-trained model cleaned a testing subset drawn from the full ShabbyPages set.

We considered the structural similarity index, a common image processing metric.

Although the ShabbyPages-trained NAFNet predicted cleaned ShabbyPages images with a lower structural similarity score than the NoisyOffice-trained model predicted cleaned RealNoisyOffice images, the ShabbyPages model achieved superior performance on RMSE and PSNR.
The ShabbyPages corpus contains a much higher feature diversity than NoisyOffice, so the lower SSIM score was not unexpected; local relationships between pixels were easier to encode for the NoisyOffice model, possibly because there are simply fewer relationships.
The RMSE and PSNR results on the other hand indicate that the ShabbyPages-trained model learned to remove more noise with higher fidelity, and more accurately predicted foreground pixels than the NoisyOffice model.
This discrepancy can partly be explained by the RealNoisyOffice corpus having been physically-noised as opposed to ShabbyPages' purely synthetic noise; small-scale features in the RealNoisyOffice testing data are not quite identical to the SimulatedNoisyOffice training inputs, while both the training and testing data taken from ShabbyPages was constructed by the same means.
In the latter case, the \textit{diversity} of the ShabbyPages data is much higher (TODO: add diversity score info), and this instance of the NAFNet may have been able to generalize over noise types better than the NoisyOffice-trained network with its lower training diversity.

The second stage of the binarization experiment was to cross-validate these models by predicting cleaned images using the other testing set as inputs.
Here, the ShabbyPages-trained model achieved all-around better performance than the NoisyOffice-trained model, with a much higher structural similarity score (0.947 vs. 0.811), lower RMSE, and higher PSNR.
This is a strong indicator that neural networks trained on ShabbyPages can outperform those trained on NoisyOffice when generalizing to other datasets.

Interestingly, the NoisyOffice-trained model RMSE and PSNR scores were lower when cleaning RealNoisyOffice than the Shabby-trained model: we were able to achieve a 3.5 decibel improvement in peak signal-to-noise ratio over the SimulatedNoisyOffice-trained model.

\section{Real Shabby Pages}
In keeping with the spirit of NoisyOffice, our original inspiration, and to provide out-of-distribution test data for other projects, we developed a separate but related dataset which we call Real ShabbyPages. To build this, we isolated reproduction steps for several physical operations, designed real-world augmentation pipelines with these, and then engaged a team of workers to use these instructions to produce several copies of printed born-digital documents with real noise.

Several challenges arose in the construction of this dataset.
Manually producing a database of noisy documents is a time-consuming and repetitive task; creating just 100 real-world documents with real noise features required the labor of several people.
Old fax machines, faulty commercial printers, and other hardware suitable for inducing the intended effects were difficult to find. Environmental concerns like the volume of paper and ink used, or the energy expenditure, also factored into our decision to pursue synthetic augmentations for ShabbyPages.

\bibliographystyle{splncs04}
\bibliography{paper}
\end{document}

